{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r''\n",
    "\n",
    "ids_path = 'prepeard_data2.csv'\n",
    "vocab_path = \"vocab.csv\"\n",
    "\n",
    "data_len = 2231142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UKN>', '<PAD>', '.', '*', ',', 'and', 'the', '1', 'to', 'in', 'a', '2', 'with', '1/2', ')', '(', 'of', 'cup', 'until', 'add', 'for', 'or', 'c.', 'minutes', 'salt', 'sugar', 'into', 'on', 'pepper', 'heat', '3', 'teaspoon', 'butter', '1/4', 'chopped', 'oil', 'over', 'mix', 'cheese', 'water', ';', 'tsp', 'flour', 'large', 'cups', '4', 'cream', 'tablespoons', 'cook', 'pan', 'bowl', 'mixture', 'stir', 'about', 'onion', 'sauce', 'at', 'bake', 'baking', 'chicken', 'can', 'place', 'garlic', 'is', 'milk', 'tbsp', 'oven', 'it', 'medium', 'cut', 'top', 'eggs', 'brown', 'juice', 'ingredients', 'from', 'combine', 'pour', 'tablespoon', 'well', 'small', 'fresh', 'egg', 'remove', '5', 'serve', '8', 'then', 'vanilla', 'ground', 'cool', 'oz', 'powder', 'cover', '6', '12', 'you', 'each', 'all', '10', 'together', 'if', 'lemon', 'sliced', 'olive', '3/4', 'remaining', 'teaspoons', 'let', 'boil', 'set', 'lb', 'sprinkle', 'green', 'onions', ':', 'taste', 'are', 'hot', 'dough', 'as', 'red', 'ounces', 'tomatoes', 'stirring', 'chocolate', 'dry', 'beat', 'white', 'bread', 'potatoes', 'skillet', 'rice', 'dish', 'simmer', 'cooked', 'half', 'minced', 'preheat', '15', 'drain', 'put', 'up', 'bring', 'out', 'ounce', 'slices', 'soup', '30', 'grated', 'black', 'melted', 'vinegar', 'pieces', 'cake', 'beef', 'your', 'cinnamon', '1/3', 'use', 'spread', 'be', 'more', 'lightly', 'finely', 'hours', 'diced', 'not', 'meat', 'make', 'aside', 'saucepan', 'when', 'beans', 'parsley', 'whole', 'tomato', 'cloves', 'degrees', 'margarine', 'them', 'an', 'pkg', 'cooking', 'soda', 'smooth', '350Â°', 'tender', 'sheet', '20', 'serving', 'before', 'this', 'spoon', 'celery', 'one', 'low', 'just', 'warm', 'hour', 'broth', 'side', 'golden', 'shredded', 'sour', 'leaves', 'roll', 'corn', 'will', 'i', 'pot', 'x', 'using', 'dried', 'drained', 'whisk', 'pie', 'season', 'bacon', '14', 'crushed', 'nuts', 'peeled', 'optional', 'wine', 'bottom', 'paper', 'saute', 'frozen', 'vegetable', 'mushrooms', 'do', 'orange', 'blend', 'turn', 'transfer', 'mustard', 'inch', 'high', 'through', 'time', 'by', 'chips', 'layer', 'slice', 'refrigerate', 'toss', '!', 'crumbs', \"'s\", 'melt', 'extract', 'sides', 'parmesan', 'thick', 'off', 'desired', 'reduce', 'pasta', 'pork', 'but', 'batter', 'ginger', 'cold', 'syrup', 'coconut', 'pineapple', 'boiling', 'peppers', 'liquid', 'garnish', 'package', 'temperature', 'greased', 'onto', 'softened', 'spray', 'beaten', 'dressing', 'minute', 'crust', 'ice', 'filling', 'carrots', 'vegetables', 'slightly', 'chill', '9', 'light', 'chili', 'all-purpose', 'coat', 'browned', 'food', 'basil', 'down', '350', 'pound', 'cheddar', 'seeds', 'soft', 'fold', 'that', 'lime', 'grill', 'mayonnaise', 'stock', 'evenly', 'seasoning', 'heavy', 'stick', 'cookie', 'pecans', 'gently', 'room', 'rack', 'so', 'soy', 'salad', '&', 'freshly', 'center', 'very', 'thinly', 'have', 'while', 'once', 'done', 'occasionally', 'foil', 'whip', 'completely', 'they', 'honey', 'sweet', 'other', 'noodles', 'stand', 'little', 'min', 'another', 'bell', 'pinch', 'plate', 'shrimp', 'cans', 'thyme', 'any', 'fat', 'prepared', '1/8', 'paste', '2/3', '-', 'casserole', 'thin', 'like', 'cilantro', 'after', 'keep', 'wrap', '7', \"''\", 'mixing', '16', 'bag', 'powdered', 'sausage', 'some', 'nutmeg', 'peel', 'enough', 'mixer', 'first', '45', 'two', 'oregano', 'covered', 'fish', 'blender', 'apples', 'medium-high', 'cubes', 'shortening', 'press', 'cornstarch', 'spinach', 'peanut', 'form', 'least', 'blended', 'allow', 'topping', 'till', 'flakes', 'cocoa', 'whites', 'has', 'fry', 'paprika', 'cumin', 'apple', 'broccoli', 'makes', 'processor', 'grease', 'zest', '13', 'size', 'g', 'plastic', 'strips', 'divided', 'refrigerator', \"n't\", 'should', 'clove', 'few', '25', 'clean', 'pudding', 'drizzle', 'continue', 'rest', 'whipped', 'brush', 'immediately', 'plus', 'container', 'yellow', 'pastry', 'unsalted', 'arrange', 'fruit', 'jar', 'potato', 'mushroom', 'thoroughly', 'raisins', 'roast', 'extra', 'overnight', 'speed', 'bay', 'chop', 'walnuts', 'constantly', 'surface', 'return', 'box', 'fork', 'microwave', 'recipe', 'almonds', 'peas', 'toasted', 'around', 'buttermilk', 'worcestershire', 'yolks', 'kosher', 'may', 'yeast', 'yogurt', 'cookies', 'italian', 'take', 'loaf', 'turkey', 'prepare', 'dash', 'pounds', 'breasts', 'lettuce', 'mixed', 'ham', 'combined', 'longer', 'zucchini', 'firm', 'ready', 'packed', 'ml', 'meanwhile', 'edges', 'gradually', 'cabbage', 'pans', 'balls', 'both', 'fill', 'knife', 'sesame', '40', 'line', 'floured', 'back', 'crackers', 'glass', 'needed', 'fine', 'coarsely', 'seconds', 'mozzarella', 'comes', 'cayenne', 'wire', 'olives', 'no', 'repeat', 'puree', 'uncovered', 'leaf', 'divide', 'shape', 'granulated', 'squash', 'slowly', 'too', 'frying', 'c', 'strawberries', 'dip', 'inches', 'drop', 'shell', 'enjoy', 'skin', 'turning', 'according', 'boneless', 'piece', 'sheets', 'instant', 'lbs', 'cubed', 'store', 'process', 'sift', '9-inch', 'f', 'marinade', 'thawed', 'additional', 'pumpkin', 'grams', 'f.', 'rolls', 'bananas', 'breast', 'separate', 'steak', 'carrot', 'possibly', 'muffin', 'used', 'except', 'parchment', 'mins', 'thickened', 'dark', 'directions', 'discard', 'mint', 'chunks', 'chilled', 'tortillas', 'wedges', 'canned', 'round', 'sure', 'next', 'crumbled', 'plain', 'seed', 'sticks', 'adding', 'reserved', 'per', 'halved', 'oats', 'ball', 'crisp', 'cooled', 'freeze', 'again', 'good', 'carefully', 'almond', '1-1/2', 'cracker', 'heated', 'rosemary', '1/2-inch', 'shallow', 'long', 'wheat', 'electric', 'halves', 'salmon', 'cherry', 'chops', 'also', 'dill', 'between', 'seeded', '1-inch', 'note', 'none', 'salsa', 'nonstick', 'spaghetti', 'deep', 'confectioners', 'graham', 'gelatin', 'servings', 'lid', 'mashed', '18', '35', 'fluffy', 'preheated', 'creamy', 'rinse', 'leave', 'approximately', 'lengthwise', 'sea', 'reserve', 'get', 'inserted', 'salted', 'sharp', '375Â°', 'tortilla', '50', 'spices', 'frosting', 'excess', 'cherries', 'bit', 'still', 'coated', 'jalapeno', 'roasted', 'seal', 'curry', 'want', 'evaporated', 'removed', 'uncooked', 'platter', 'condensed', 'chives', 'squares', 'juices', '24', 'amount', 'flavor', 'spatula', 'coffee', '400', 'marshmallows', 'rinsed', 'necessary', '375', 'shallots', 'beating', 'pizza', 'virgin', 'rise', 'whipping', 'scallions', 'wash', 'only', 'glaze', 'oleo', \"'\", 'skinless', 'dissolved', 'layers', 'cider', 'coriander', 'pat', 'serves', 'unsweetened', 'head', 'ripe', 'several', 'work', 'dissolve', 'double', 'sifted', 'shake', 'asparagus', 'canola', 'qt', 'towels', 'hand', 'knead', 'consistency', 'need', 'hamburger', 'ribs', 'bean', 'freezer', 'sprigs', 'ketchup', 'towel', 'strain', 'same', 'sit', 'toast', 'fillets', 'under', 'cucumber', 'spice', 'hands', '``', 'macaroni', 'stiff', 'shells', 'rub', '13-inch', 'bunch', 'frequently', 'toothpick', 'coarse', '34', 'mash', 'such', 'pink', 'break', 'apart', '400Â°', 'see', 'square', 'wooden', '--', 'day', 'start', 'times', 'jello', 'tops', 'baby', 'dutch', 'maple', 'cranberries', 'breadcrumbs', 'last', 'buttered', 'dijon', 'raw', 'chilies', 'almost', 'stuffing', 'bouillon', 'avocado', 'ungreased', '325Â°', 'baked', 'lamb', 'scoop', 'seasoned', 'middle', 'full', 'banana', 'tightly', 'greens', 'peaches', 'bowls', 'end', 'stems', 'eggplant', 'seasonings', 'along', 'soak', 'icing', 'french', 'much', '%', 'trimmed', '2-3', 'made', 'making', 'flat', 'roasting', 'cutting', 'dice', 'bubbly', 'rind', 'cornmeal', 'cauliflower', 'broiler', 'balsamic', 'peanuts', 'blueberries', 'often', 'color', 'slow', 'extra-virgin', 'quartered', 'quart', 'medium-low', 'days', 'edge', 'than', 'cooker', 'bits', 'taco', 'adjust', 'sage', 'even', 'best', 'reduced', 'jars', 'lay', 'berries', 'sherry', 'steaks', 'biscuits', 'broil', 'plates', 'yolk', 'strawberry', 'ricotta', 'sweetened', 'marinate', 'been', 'oranges', 'squeeze', 'tuna', 'substitute', '350f', 'leaving', 'bottle', 'fridge', 'chile', 'pulse', 'smoked', 'scrape', 'rum', 'without', 'now', 'batches', 'molasses', 'herbs', 'lower', 'stalks', 'inside', 'cottage', 'allspice', 'quickly', 'roughly', 'candy', '2-inch', 'my', 'way', 'fennel', 'patties', 'second', 'meal', 'board', 'firmly', 'caramel', 'refrigerated', 'peaks', 'soften', 'airtight', 'three', 'chiles', 'jack', 'its', 'wet', 'pitted', 'ahead', 'rolling', 'everything', 'lean', 'four', 'gravy', 'favorite', 'begin', '60', 'feta', 'lined', 'away', 'choice', 'steam', 'packages', 'jam', 'less', 'open', 'begins', 'tofu', '1/4-inch', 'kraft', 'every', 'tea', 'wax', 'weight', 'meatballs', 'these', 'which', 'ends', 'thermometer', 'cereal', 'jell-o', 'aluminum', 'split', 'crumble', 'tart', 'depending', '350Â°f', 'step', 'tin', 'jelly', 'swiss', 'part', 'tray', 'single', 'addition', 'thickens', 'directed', 'lowfat', 'there', 'cucumbers', 'skim', 'barbecue', 'run', 'trim', '100', 'thicken', 'tbs', 'bars', 'drippings', '1-2', 'coloring', 'dates', 'stove', 'crisco', 'thickness', 'their', 'wok', 'flip', 'melts', 'shallot', 'fried', 'creamed', 'turmeric', 'regular', 'r', 'low-fat', 'beer', 'hard', 'raspberries', 'whisking', 'cored', 'racks', 'flatten', 'slotted', 'rings', '11', '8-inch', 'equal', 'reserving', 'great', 'buns', 'pt', 'sprouts', 'boiled', 'free', 'semi-sweet', '3-4', 'crispy', 'kidney', 'crab', 'incorporated', 'custard', 'brand', 'veggies', 'does', 'forms', 'added', 'rounds', 'absorbed', 'sandwich', 'bite', 'prevent', 'dust', 't', 'tube', 'cube', 'applesauce', 'creme', 'tabasco', 'waxed', 'preferably', 'right', 'tartar', 'cocktail', 'base', 'fully', 'oatmeal', 'alternately', 'tip', 'grain', '200', 'pears', 'fragrant', 'couple', 'moderate', 'wide', 'check', 'tarragon', 'cakes', 'translucent', 'easily', 'simmering', 'among', 'lentils', \"'re\", 'point', 'stir-fry', 'peppercorns', 'cranberry', 'velveeta', 'possible', 'sized', 'come', 'rolled', 'undrained', 'parts', 'quarts', 'lemons', 'hold', 'measure', 'nice', 'clear', 'boiler', 'crosswise', 'separated', '325', 'removing', 'eat', 'mini', 'mold', 'flavoring', 'starts', 'drops', 'total', 'grate', 'big', 'circle', 'blue', 'chestnuts', '10-inch', 'monterey', 'flavors', 'liqueur', 'pure', 'beets', 'pine', 'above', 'capers', 'finish', 'washed', 'punch', 'raspberry', 'fat-free', 'kitchen', 'mango', 'careful', 'increase', 'vinaigrette', 'spring', 'halfway', 'pint', '$', 'ladle', 'sugars', 'broken', 'quick', 'fingers', 'generously', 'horseradish', 'we', 'leeks', 'cutter', 'sieve', 'scraping', 'during', 'yield', 'self-rising', 'degree', 'give', 'muffins', 'most', 'better', 'left', 'instructions', 'scallops', 'wings', 'unbaked', 'loosely', 'coating', 'save', 'half-and-half', 'prefer', 'thru', 'brandy', 'non-stick', '2-quart', 'ranch', 'uncover', 'biscuit', 'bones', 'dozen', 'doubled', 'working', 'flaked', 'rectangle', 'mild', 'served', 'mayo', 'individual', 'yields', '28', 'stew', 'starting', 'burn', 'skewers', 'goat', '12-inch', 'bone', 'pesto', 'torn', 'close', 'iron', 'cooling', 'five', 'crumb', 'sodium', 'apricot', 'either', 'artichoke', 'al', 'quarters', 'clams', 'crusts', 'crush', 'metal', 'portion', 'touch', 'pull', 'peach', '175', 'thighs', '15-20', 'dente', 'florets', 'preserves', 'cheeses', 'kale', 'moistened', '23', 'carton', 'arugula', 'would', 'pecan', 'moist', 'glasses', 'catsup', 'chickpeas', 'pressing', 'delicious', 'warmed', 'sticky', 'heaping', 'skins', 'bags', 'bubbles', 'colander', 'handle', '300', 'go', 'hearts', 'how', 'portions', 'order', 'chipotle', '425', 'style', 'plum', '250', 'meringue', 'reaches', 'cardamom', 'quinoa', 'slivered', 'old', '-2', 'measuring', 'grind', 'chuck', 'crescent', 'frost', 'squeezed', 'third', 'fit', 'packet', 'shred', 'grapes', 'strainer', 'crumbly', 'marshmallow', 'miniature', 'stuff', 'vodka', 'apricots', '10-15', 'loaves', 'dipping', 'partially', 'concentrate', 'leftover', 'cracked', 'purpose', 'etc', 'running', '8-10', 'pack', 'de', 'herb', 'across', 'greek', 'create', '300Â°', 'grape', 'veal', 'really', 'look', 'outside', 'deveined', 'puff', 'mandarin', 'become', 'loosen', 'rhubarb', 'spicy', 'med', 'try', 'couscous', 'beginning', 'pulp', 'invert', 'sprayed', 'lift', '-3', 'crabmeat', '450', '9x13', 'wafers', '425Â°', 'semisweet', 'machine', 'easy', 'later', 'fitted', 'lasagna', 'kernels', 'because', 'texture', 'flavored', 'handful', 'jalapenos', 'crock', 'bisquick', 'doneness', 'envelope', 'cleaned', 'english', '4-5', 'dishes', 'larger', 'oiled', 'bundt', 'root', 'soaked', 'griddle', 'fluid', 'usually', 'becomes', 'was', 'tins', 'lukewarm', 'stem', 'sandwiches', 'level', 'ale', 'stalk', 'mexican', 'sprig', 'dissolves', 'original', 'dredge', 'assemble', 'rimmed', 'fillet', 'twice', 'dot', 'pancake', 'tenderloin', 'setting', 'decorate', '20-25', 'cornbread', 'smaller', 'panko', 'leek', 'flesh', \"'ll\", 'table', 'duck', 'help', 'real', 'thaw', 'candied', 'grilled', 'croutons', 'hash', 'instead', 'dessert', 'sticking', 'moderately', 'pin', 'swirl', 'nonfat', 'lumps', 'tossing', 'juiced', 'core', 'angel', '02/09', 'entire', 'cupcakes', 'poppy', 'avocados', 'holes', 'new', 'weeks', 'pita', 'romaine', 'skewer', 'quarter', 'being', 'bubbling', 'cheesecake', 'eagle', 'smoke', 'artichokes', 'attachment', 'rubber', 'chinese', 'pwdr', 'wilted', 'follows', '500', 'pickle', 'resembles', 'active', 'sunflower', 'pear', 'prosciutto', 'ritz', 'sausages', 'diameter', 'homemade', 'pick', 'lemonade', 'short', 'american', 'basting', 'springform', 'slicing', 'topped', 'generous', 'pressure', 'below', 'filled', 'own', '150', 'pickles', '32', 'reheat', 'phyllo', 'six', 'rounded', 'ramekins', 'marjoram', 'position', 'cm', 'pancakes', 'air', 'oyster', 'maraschino', 'cashews', 'butterscotch', 'toppings', 'finished', 'crisp-tender', 'pepperoni', 'browns', 'flame', 'seedless', 'dinner', 'saffron', 'chunky', 'pam', 'relish', '6-8', 'flat-leaf', 'bath', 'miracle', 'pimento', 'unpeeled', 'grnd', 'scatter', '450Â°', 't.', 'stewed', 'weed', 'wild', 'release', 'cupcake', 'pale', 'turns', 'skinned', 'walnut', 'scallion', 'starch', 'reduced-fat', 'week', 'strip', 'browning', 'loin', 'works', 'drink', 'kernel', 'diagonally', '375f', 'calories', 'bite-size', '...', 'popcorn', 'test', 'could', 'limes', '-4', 'philadelphia', 'barley', 'sprinkling', 'bourbon', 'hrs', 'find', 'and/or', 'organic', 'undiluted', '10-12', 'dollop', 'pureed', 'wipe', 'sauerkraut', 'bulk', 'rim', 'briefly', 'polenta', 'burgers', 'hazelnuts', 'pop', 'baguette', '400f', 'nicely', 'circles', 'mussels', 'ovenproof', 'pancetta', 'pipe', 'low-sodium', '1-1/4', 'tie', '400Â°f', 'sear', 'pressed', '55', 'shelled', 'simple', 'whatever', 'crusty']\n"
     ]
    }
   ],
   "source": [
    "with open(base_path + vocab_path, 'r') as file:\n",
    "    vocab = [t[0] for t in list(csv.reader(file))[1:]]\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab =  vocab + [\"<ING>\", \"<REC>\"]\n",
    "\n",
    "\n",
    "\n",
    "w2id =  {w:i for i,w in  enumerate(vocab)}\n",
    "id2w =  {i:w for i,w in  enumerate(vocab)}\n",
    "\n",
    "(w2id)[\"<REC>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: torch.Size([32, 10])\n",
      "Output vector shape: torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "class Generator_next_token(nn.Module):\n",
    "    def __init__(self, vocab_len, hidden_size, num_layers=1):\n",
    "        super(Generator_next_token, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_len, embedding_dim=hidden_size)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(num_layers * hidden_size, vocab_len)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        embedded = self.embedding(input_sequence)\n",
    "        # input_sequence shape: (batch_size, sequence_length, input_size)\n",
    "        hidden_states, (h_n, _) = self.rnn(embedded)\n",
    "        # h_n shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        # Concatenate or stack hidden states from all layers\n",
    "        h_n_flat = h_n.permute(1, 0, 2).contiguous().view(input_sequence.size(0), -1)\n",
    "        \n",
    "        # Pass through the linear layer\n",
    "\n",
    "        output = self.fc(h_n_flat)\n",
    "        # output shape: (batch_size, output_size)\n",
    "       \n",
    "        return output, hidden_states\n",
    "\n",
    "# Example usage\n",
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "input_size = 5\n",
    "hidden_size = 64\n",
    "output_size = input_size\n",
    "\n",
    "# Create an instance of the Generator\n",
    "generator = Generator_next_token(input_size, hidden_size, 2)\n",
    "\n",
    "# Generate a random batch of input sequences\n",
    "input_sequence = torch.randint(0,input_size,(batch_size, sequence_length))\n",
    "\n",
    "# Forward pass through the generator\n",
    "output_vector, _ = generator(input_sequence)\n",
    "\n",
    "print(\"Input sequence shape:\", input_sequence.shape)\n",
    "print(\"Output vector shape:\", output_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_len,hidden_size, depth,max_len):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(vocab_len, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers=depth, batch_first=True)\n",
    "        self.hidden2vocab = nn.Linear(hidden_size, vocab_len)\n",
    "        self.hidden_l = nn.Linear(hidden_size * depth, hidden_size)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        original_input = input_seq.clone()\n",
    "        sequences = self.embedding(input_seq.float())\n",
    "        outputs = []\n",
    "    \n",
    "        for _ in range(self.max_len):\n",
    "            last_hidden, hidden_states = self.singel_pass(sequences)\n",
    "            outputs.append(last_hidden)\n",
    "            sequences = torch.cat((sequences, last_hidden.unsqueeze(1)), dim = 1)\n",
    "       \n",
    "        outputs = torch.stack(outputs, dim = 1)\n",
    "        vocab_vectors = self.hidden2vocab(outputs)\n",
    "        vocab_probs = torch.softmax(vocab_vectors, dim=2)\n",
    "        \n",
    "        out = torch.concat((original_input,vocab_probs), dim=1)\n",
    "        return out, hidden_states\n",
    "            \n",
    "        \n",
    "\n",
    "    def singel_pass(self, input_seq):\n",
    "        rand_source = torch.rand((input_seq.shape[0], 1, input_seq.shape[2])).to(device)\n",
    "        #embedded = torch.concat((rand_source, embedded), dim = 1)\n",
    "        hidden_states, (last_hidden, _) = self.rnn(input_seq)\n",
    "        last_hidden = last_hidden.view(1, -1)\n",
    "        output = self.hidden_l(last_hidden)\n",
    "        \n",
    "        return output, hidden_states\n",
    "\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_len, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_len, embedding_dim=hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        hidden_states, (last_hidden, _) = self.rnn(embedded)\n",
    "        output = self.fc(last_hidden)\n",
    "        prob = F.sigmoid(output)\n",
    "        return prob, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataLoader:\n",
    "    def __init__(self, csv_file_path, batch_size, num_rows, start = 0):\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_rows = num_rows\n",
    "        self.current_index = 0\n",
    "        self.file_handle = None\n",
    "        self.csv_writer = None\n",
    "        self.headers = None\n",
    "\n",
    "    def open_csv_file(self):\n",
    "        self.file_handle = open(self.csv_file_path, 'r', newline='')\n",
    "        self.csv_writer = csv.reader(self.file_handle)\n",
    "        # Assuming the first row contains headers\n",
    "        self.headers = next(self.csv_writer)\n",
    "\n",
    "    def close_csv_file(self):\n",
    "        if self.file_handle is not None and not self.file_handle.closed:\n",
    "            self.file_handle.close()\n",
    "\n",
    "    def reset(self):\n",
    "        self.close_csv_file()\n",
    "        self.open_csv_file()\n",
    "        self.current_index = 0\n",
    "\n",
    "    def has_next_batch(self):\n",
    "        return self.current_index < self.num_rows\n",
    "\n",
    "    def get_next_batch(self, pad):\n",
    "        ing_batch = [] \n",
    "        rec_batch = []\n",
    "        for _ in range(self.batch_size):\n",
    "            if not self.has_next_batch():\n",
    "                break\n",
    "            ing, rec = next(self.csv_writer)\n",
    "            ing_batch.append(torch.tensor(eval(ing)).to(device))\n",
    "            rec_batch.append(torch.tensor(eval(rec)).to(device))\n",
    "            self.current_index += 1\n",
    "       \n",
    "        \n",
    "\n",
    "        ing_padded = pad_sequence(ing_batch, batch_first=True, padding_value=pad)\n",
    "        rec_padded = pad_sequence(rec_batch, batch_first=True, padding_value=pad)\n",
    "        return ing_padded, rec_padded\n",
    "    \n",
    "    def batches_left(self):\n",
    "        return (self.num_rows - self.current_index) // self.batch_size\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.close_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n",
      "tensor([[0.1826, 0.3651, 0.9129]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,100,10):\n",
    "    print(F.normalize(torch.tensor([[0.1,0.2,0.5]], dtype=torch.float32) * i, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'args = torch.randint(1,100, (1000, 3))\\nr = torch.arange(2500).view(50, -1)\\nfor b,c,d in tqdm(args):\\n    h = one_hot_noisify_tokens(r, 2500, b,c,30).argmax(dim=2)\\n    assert(torch.equal(r, h))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"args = torch.randint(1,100, (1000, 3))\n",
    "r = torch.arange(2500).view(50, -1)\n",
    "for b,c,d in tqdm(args):\n",
    "    h = one_hot_noisify_tokens(r, 2500, b,c,30).argmax(dim=2)\n",
    "    assert(torch.equal(r, h))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_batch(ings, start, midel):\n",
    "    start_tokens = torch.full((ings.shape[0],), start).unsqueeze(1).to(device)\n",
    "    midel_tokens = torch.full((ings.shape[0],), midel).unsqueeze(1).to(device)\n",
    "    return torch.cat((start_tokens, ings, midel_tokens), dim=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1500,    7,   80,  434,    0,  155,    4,   69,  142,    3,   45,    0,\n",
       "          59,  472,    3,    7,   60,   46,   16,  436,  147,    3,    7, 1167,\n",
       "         204,   46,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1, 1501], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r\"prepeard_data2.csv\"\n",
    "data_loader = CSVDataLoader(data_path, 32, 2231142)\n",
    "data_loader.open_csv_file()\n",
    "d = data_loader.get_next_batch(w2id[\"<PAD>\"])\n",
    "data_loader.close_csv_file()\n",
    "\n",
    "patch_batch(d[0], w2id[\"<ING>\"], w2id[\"<REC>\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy_loader:\n",
    "    def __init__(self, d) -> None:\n",
    "        self.d = d\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def open_csv_file(self):\n",
    "        pass\n",
    "    def batches_left(self):\n",
    "        return 1\n",
    "\n",
    "    def get_next_batch(self, a):\n",
    "        return self.d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,num_layers, dropout_rate):\n",
    "        super(NextTokenPredictor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_hidden = nn.Parameter(torch.zeros(1, 1, hidden_dim), requires_grad=True)\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.lstm(embedding)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "generator_hidden_size = 3048\n",
    "embed_size = 1500\n",
    "generator_depth = 4\n",
    "drop_rate = 0.8\n",
    "\n",
    "gen_lr  = 0.001\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "\n",
    "seq_len = 20\n",
    "\n",
    "nr_batches = 200\n",
    "\n",
    "test_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#a = CSVDataLoader(r\"C:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\dummy_data.csv\", 1, 1)\n",
    "#a.open_csv_file()\n",
    "#pad1 = w2id[\"<PAD>\"]\n",
    "\n",
    "#dummy_loader = Dummy_loader(a.get_next_batch(pad1))\n",
    "data_loader = CSVDataLoader(data_path, batch_size, data_len - test_size)\n",
    "\n",
    "\n",
    "#data_loader = dummy_loader\n",
    "\n",
    "\n",
    "data_loader.open_csv_file()\n",
    "# Instantiate Generator and Discriminator\n",
    "generator = NextTokenPredictor(len(vocab),embed_size, generator_hidden_size, generator_depth, drop_rate).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=gen_lr)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(gen_optimizer,mode='min',factor=0.8, patience=3, threshold=0.00005)\n",
    "max_grad_norm = 1.0\n",
    "#torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_grad_norm)\n",
    "#torch.nn.utils.clip_grad_norm_(generator.parameters(), max_grad_norm)\n",
    "\n",
    "\n",
    "avg_gen_losses = []\n",
    "\n",
    "\n",
    "if not os.path.exists(\"simpel_model.pth\"):\n",
    "    for epoch in (range(epochs)):\n",
    "        \n",
    "        data_loader.reset()\n",
    "        batch_nr = 0\n",
    "        total_nr_batches = data_loader.batches_left()\n",
    "        #total_nr_batches =  100\n",
    "        itr =(range(total_nr_batches))\n",
    "        progress_bar = tqdm(total=nr_batches)\n",
    "        for batch_nr in itr:\n",
    "            if batch_nr == nr_batches:\n",
    "                break\n",
    "            \n",
    "            # Training the Discriminator\n",
    "            pad1 = w2id[\"<PAD>\"]\n",
    "        \n",
    "    \n",
    "            real_ingredients, real_recipies = data_loader.get_next_batch(pad1)\n",
    "            real_patched_ings = patch_batch(real_ingredients, \n",
    "                                                w2id[\"<ING>\"],\n",
    "                                                w2id[\"<REC>\"])\n",
    "            \n",
    "            #Generator input does not need to be smoothend\n",
    "            real_batch = torch.concat((real_patched_ings,\n",
    "                                        real_recipies), dim = 1)\n",
    "\n",
    "        \n",
    "            current_seq_len = min(seq_len,real_batch.shape[1])\n",
    "            gen_losses = []\n",
    "            for index in range(0,real_batch.shape[1]- seq_len - 1, seq_len):\n",
    "                current_seq_len = min(current_seq_len, real_batch.shape[1]-index)\n",
    "                #current_seq_len = seq_len\n",
    "                gen_optimizer.zero_grad()\n",
    "                sequence_x = real_batch[:, index:index+current_seq_len]   \n",
    "                tokens_y = torch.nn.functional.one_hot(real_batch[:, index+1:index+current_seq_len+1]  , len(vocab))\n",
    "                \n",
    "                generated_logits = generator(sequence_x)\n",
    "                \n",
    "                gen_loss = criterion(generated_logits, tokens_y.float()) / current_seq_len\n",
    "                gen_loss.backward()\n",
    "                gen_optimizer.step()\n",
    "                gen_losses.append(gen_loss.item())\n",
    "            batch_avg_loss = sum(gen_losses) / len(gen_losses)\n",
    "            lr_scheduler.step(batch_avg_loss)\n",
    "            avg_gen_losses.append(batch_avg_loss)\n",
    "            progress_bar.update(1)\n",
    "    torch.save(generator.state_dict(), 'simpel_model.pth')\n",
    "    torch.save(avg_gen_losses, 'simpel_model_losses.pth')\n",
    "        \n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'simpel_model_losses.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m generator \u001b[38;5;241m=\u001b[39m NextTokenPredictor(\u001b[38;5;28mlen\u001b[39m(vocab),embed_size, generator_hidden_size, generator_depth, drop_rate)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m generator\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimpel_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m avg_gen_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimpel_model_losses.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KG\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\KG\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\KG\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'simpel_model_losses.pth'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "generator = NextTokenPredictor(len(vocab),embed_size, generator_hidden_size, generator_depth, drop_rate).to(device)\n",
    "generator.load_state_dict(torch.load('simpel_model.pth'))\n",
    "avg_gen_losses = torch.load('simpel_model_losses.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'gen_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(avg_gen_losses, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgen_losses\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gen_losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(avg_gen_losses, color = \"red\")\n",
    "plt.show()\n",
    "gen_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:11<00:00,  5.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "475.9781254483984"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torcheval.metrics.text import Perplexity\n",
    "test_batch_size = 1\n",
    "data_loader = CSVDataLoader(data_path, test_batch_size, data_len - test_size)\n",
    "data_loader.open_csv_file()\n",
    "perplexities = []\n",
    "with  tqdm(range(int(test_size / test_batch_size))) as itr:\n",
    "    with torch.no_grad():\n",
    "        metric=Perplexity().to(device)\n",
    "        perplexities = []\n",
    "        for i in itr:  # Use your validation data loader\n",
    "            real_ingredients, real_recipies = data_loader.get_next_batch(w2id[\"<PAD>\"])\n",
    "            real_ingredients, real_recipies = real_ingredients.to(device), real_recipies.to(device)\n",
    "            real_patched_ings = patch_batch(real_ingredients, \n",
    "                                                w2id[\"<ING>\"],\n",
    "                                                w2id[\"<REC>\"])\n",
    "            real = torch.concat((real_patched_ings,\n",
    "                                        real_recipies), dim = 1).to(device)\n",
    "            real_x = real[:,:-1]\n",
    "            real_y = real[:,1:].to(device)\n",
    "            outputs = generator(real_x).to(device)\n",
    "            #print(outputs.shape, real_y.shape)\n",
    "            metric.update(outputs, real_y)\n",
    "            perplexity = metric.compute()\n",
    "            perplexities.append(perplexity.item())\n",
    "            # calculating perplexity    \n",
    "\n",
    "sum(perplexities) / len(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(prompt, dic):\n",
    "    return [dic.get(t.lower(),dic[\"<UKN>\"]) for t in word_tokenize(prompt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 733, 64, 209, 0, 10, 90, 855]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"1. break milk I brew a cool candy\", w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(answer, dic):\n",
    "    return \" \".join([dic[i] for i in answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we bake a <UKN> cake'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"we bake a NLP cake\", w2id), id2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(next_logits, k):\n",
    "    next_probs = torch.softmax(next_logits, dim=1)\n",
    "    topk_values, topk_indices = torch.topk(next_probs, k, dim=1)\n",
    "    topk_values, topk_indices = topk_values.to(device), topk_indices.to(device)\n",
    "    topk_index_indices = torch.multinomial((topk_values), 1).squeeze(1).to(device)\n",
    "    next_tokens = topk_indices[\n",
    "        torch.arange(topk_index_indices.shape[0]), topk_index_indices]\n",
    "    return next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, generator, gen_len, strat):\n",
    "    t = torch.tensor(encode(prompt, w2id)).unsqueeze(0).to(device)\n",
    "    batch = (patch_batch(t,\n",
    "                                w2id[\"<ING>\"],\n",
    "                                w2id[\"<REC>\"]))\n",
    "    for _ in range(gen_len):\n",
    "        answer = generator(batch)[:,-1, :]\n",
    "        valid_choices = answer[:,1:-2]\n",
    "        new_token = strat(valid_choices) + 1\n",
    "        if id2w[new_token.item()] == \"<PAD>\":\n",
    "            break\n",
    "        batch = torch.concat((batch, new_token.unsqueeze(1)), dim = 1)\n",
    "        \n",
    "\n",
    "    return decode([i.item() for i in batch[0]], id2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teaspoon drain will of'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([31, 140, 208, 16], id2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cm', 'pancakes', 'air', 'oyster', 'maraschino', 'cashews', 'butterscotch', 'toppings', 'finished', 'crisp-tender', 'pepperoni', 'browns', 'flame', 'seedless', 'dinner', 'saffron', 'chunky', 'pam', 'relish', '6-8', 'flat-leaf', 'bath', 'miracle', 'pimento', 'unpeeled', 'grnd', 'scatter', '450Â°', 't.', 'stewed', 'weed', 'wild', 'release', 'cupcake', 'pale', 'turns', 'skinned', 'walnut', 'scallion', 'starch', 'reduced-fat', 'week', 'strip', 'browning', 'loin', 'works', 'drink', 'kernel', 'diagonally', '375f', 'calories', 'bite-size', '...', 'popcorn', 'test', 'could', 'limes', '-4', 'philadelphia', 'barley', 'sprinkling', 'bourbon', 'hrs', 'find', 'and/or', 'organic', 'undiluted', '10-12', 'dollop', 'pureed', 'wipe', 'sauerkraut', 'bulk', 'rim', 'briefly', 'polenta', 'burgers', 'hazelnuts', 'pop', 'baguette', '400f', 'nicely', 'circles', 'mussels', 'ovenproof', 'pancetta', 'pipe', 'low-sodium', '1-1/4', 'tie', '400Â°f', 'sear', 'pressed', '55', 'shelled', 'simple', 'whatever', 'crusty', '<ING>']\n"
     ]
    }
   ],
   "source": [
    "print(list(w2id.keys())[-100:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sugar , place * 1/4 tsp c. remove beef * 1/8 tsp . remove juice ( 1 large pour powder soup\n",
      " bake and bake with ingredients for 2 c. pour , remove into * 3 c. then sauce , put * 3 tbsp minutes oz , let can mix , bake at for 2 hour minutes . bake with 350Â° and sprinkle well minutes . pour 2 minutes minutes , bake powder * 1/4 tbsp minutes until , bake with and spread 2 1/4 minutes . remove can ) , add and bake 2 minutes ; sprinkle 1 3 minutes . bake with cheese for 2 minutes minutes oz . place can heat with for 1 1/2 tsp . pour sauce\n",
      " then , sprinkle powder and stir 1 minutes of pour . add 2 ingredients , pour with * 2 minutes sugar until , stir * 1/2 tbsp . pour with ( 2 minutes ) , sprinkle * 1 c. sugar juice\n",
      " butter , remove at a 1/2 tsp hours minutes * 2 hour or oz , stir powder 2-3 . bake into ingredients and pour 2 1 large pure ) . bake into a 1 hour c. spread cheese * 1 c. let sauce soup * 1 minutes milk ( 1 tsp . bake pkg a oz . remove into and pour at 1 tsp tbsp minutes . add can bake * 1/4 tsp minutes ground bake add and stir 1/4 1 minutes minutes , place powder . sprinkle with for 1/2 large minutes of bake * 2 hour . remove\n",
      " let until a minutes minutes , add * 1/4 c. then sugar reserving and stir in 1/4 minutes . remove well ingredients . bake at ingredients 1/2 hour oz minutes ) . pour powder , bake and bake 1 1/2 large mayonnaise , put powder and pour over . bake can 350Â° ingredients * 1 1/2 minutes minutes sauce baguette 02/09 . bake well 350Â° ingredients minutes * 1 large pour , add cheese * 1 tsp lb c. ) into * 2 minutes mayonnaise ( 3/4 . bake pkg ingredients bake , stir cheese , bake and bake 1\n",
      "\n",
      "In 12-inch nonstick skillet, heat oil 1 minute on medium. Sprinkle chicken with oregano, 1/4 teaspoon salt, and 1/4 teaspoon freshly ground black pepper, and toss to coat. Add chicken to skillet and cook 5 to 7 minutes or until chicken is no longer pink throughout. Transfer chicken to plate or bowl; cover with foil to keep warm. Meanwhile, make tahini sauce: In small bowl, with wire whisk, combine tahini, water, yogurt, lemon juice, and garlic. Stir in cilantro, ground red pepper, 1/4 teaspoon salt, and 1/4 teaspoon freshly ground black pepper. Makes about 3/4 cup tahini sauce. To serve, cut off one-fourth of each pita and save for use another day. Carefully open each pita pocket and fill with one-fourth each romaine, cucumber, tomato, chicken, and green onions. Top with tahini sauce; serve gyros with any additional sauce on the side.\n"
     ]
    }
   ],
   "source": [
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "ing, rec = \"1 tsp. olive oil * 1 1/2 lb. skinless, boneless chicken-breat halves * 1/2 tsp. dried oregano * salt and pepper * 1/4 c. tahini * 1/4 c. water * 3 tbsp. nonfat plain yogurt * 1 tbsp. fresh lemon juice * 1 clove garlic * 1/4 c. Cilantro leaves * 1/4 tsp. ground red pepper (cayenne) * 4 pitas * 2 c. romaine lettuce * 1/2 seedless cucumber * 1 large tomato * 2 green onions\",\"In 12-inch nonstick skillet, heat oil 1 minute on medium. Sprinkle chicken with oregano, 1/4 teaspoon salt, and 1/4 teaspoon freshly ground black pepper, and toss to coat. Add chicken to skillet and cook 5 to 7 minutes or until chicken is no longer pink throughout. Transfer chicken to plate or bowl; cover with foil to keep warm. Meanwhile, make tahini sauce: In small bowl, with wire whisk, combine tahini, water, yogurt, lemon juice, and garlic. Stir in cilantro, ground red pepper, 1/4 teaspoon salt, and 1/4 teaspoon freshly ground black pepper. Makes about 3/4 cup tahini sauce. To serve, cut off one-fourth of each pita and save for use another day. Carefully open each pita pocket and fill with one-fourth each romaine, cucumber, tomato, chicken, and green onions. Top with tahini sauce; serve gyros with any additional sauce on the side.\"\n",
    "\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(((generate(\"Banana\", generator, 100, lambda p: topk(p,8))).split(\"<REC>\")[1]))\n",
    "print()\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
