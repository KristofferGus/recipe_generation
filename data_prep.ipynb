{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrmrk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\\\\"\n",
    "original_cvs_name = r\"RecipeNLG_dataset.csv\"\n",
    "prepeard_data_name = \"prepeard_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2231142it [00:50, 43751.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1 c. firmly packed brown sugar * 1/2 c. evaporated milk * 1/2 tsp. vanilla * 1/2 c. broken nuts (pecans) * 2 Tbsp. butter or margarine * 3 1/2 c. bite size shredded rice biscuits',\n",
       " 'In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine. Stir over medium heat until mixture bubbles all over top. Boil and stir 5 minutes more. Take off heat. Stir in vanilla and cereal; mix well. Using 2 teaspoons, drop and shape into 30 clusters on wax paper. Let stand until firm, about 30 minutes.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bads = ['\\x00']\n",
    "data = []\n",
    "\n",
    "def fix_text(line, j):\n",
    "    return j.join([r for r in json.loads(line) if not all([r.__contains__(bad) for bad in bads])])\n",
    "\n",
    "with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        csv_reader = csv.reader(input_csvfile)\n",
    "        next(csv_reader, None)\n",
    "\n",
    "        # Copy selected columns from the input file to the output file\n",
    "        for row in tqdm(csv_reader):\n",
    "            data.append([fix_text(row[2], \" * \"),fix_text(row[3], \" \")])\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'c.', 'firmly', 'packed', 'brown', 'sugar', '*', '1/2', 'c.', 'evaporated', 'milk', '*', '1/2', 'tsp', '.', 'vanilla', '*', '1/2', 'c.', 'broken', 'nuts', '(', 'pecans', ')', '*', '2', 'Tbsp', '.', 'butter', 'or', 'margarine', '*', '3', '1/2', 'c.', 'bite', 'size', 'shredded', 'rice', 'biscuits']\n",
      "['In', 'a', 'heavy', '2-quart', 'saucepan', ',', 'mix', 'brown', 'sugar', ',', 'nuts', ',', 'evaporated', 'milk', 'and', 'butter', 'or', 'margarine', '.', 'Stir', 'over', 'medium', 'heat', 'until', 'mixture', 'bubbles', 'all', 'over', 'top', '.', 'Boil', 'and', 'stir', '5', 'minutes', 'more', '.', 'Take', 'off', 'heat', '.', 'Stir', 'in', 'vanilla', 'and', 'cereal', ';', 'mix', 'well', '.', 'Using', '2', 'teaspoons', ',', 'drop', 'and', 'shape', 'into', '30', 'clusters', 'on', 'wax', 'paper', '.', 'Let', 'stand', 'until', 'firm', ',', 'about', '30', 'minutes', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(data[0][0]))\n",
    "print(word_tokenize(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 585544/2231142 [33:51<1:35:09, 288.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ingrent_tokalized:\n\u001b[0;32m      7\u001b[0m     vocab_ingredients[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m recipie_tokalized \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipie\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(recipie):\n\u001b[0;32m     10\u001b[0m     vocab_recipies[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\nltk\\tokenize\\destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[1;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_ingredients = Counter()\n",
    "vocab_recipies = Counter()\n",
    "data_tokanized = []\n",
    "for (ingrent_list, recipie) in tqdm(data):\n",
    "    ingrent_tokalized = word_tokenize(ingrent_list)\n",
    "    for word in ingrent_tokalized:\n",
    "        vocab_ingredients[word] += 1\n",
    "    recipie_tokalized = word_tokenize(recipie)\n",
    "    for word in word_tokenize(recipie):\n",
    "        vocab_recipies[word] += 1\n",
    "    data_tokanized.append((ingrent_tokalized, recipie_tokalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(base_path + prepeard_data_name):\n",
    "    with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        with open(base_path + prepeard_data_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            next(csv_reader, None)\n",
    "\n",
    "            csv_writer.writerow([\"X\",\"Y\"])\n",
    "\n",
    "            # Copy selected columns from the input file to the output file\n",
    "            for row in csv_reader:\n",
    "                selected_row = [fix_text(row[2], \" * \"),fix_text(row[3], \" \")]\n",
    "                csv_writer.writerow(selected_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
