{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrmrk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveOnIterate:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.original_len = len(data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # This method should return the iterator object (in this case, self)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Check if there are more elements in the data\n",
    "        if self.data:\n",
    "            # Get and remove the next element from the data\n",
    "            result = self.data.pop(0)\n",
    "            return result\n",
    "        else:\n",
    "            # No more elements, raise StopIteration\n",
    "            raise StopIteration\n",
    "    def __len__(self):\n",
    "        return self.original_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\\\\"\n",
    "original_cvs_name = r\"RecipeNLG_dataset.csv\"\n",
    "prepeard_data_name = \"prepeard_data.csv\"\n",
    "ingredient_vocab_name = \"ingredient_vocab.csv\"\n",
    "recipie_vocab_name = \"recipie_vocab.csv\"\n",
    "\n",
    "data_name = \"data.csv\"\n",
    "data_tokanized_name = \"data_tokanized.csv\"\n",
    "\n",
    "ing_count_name = \"ingredient_count.pth\"\n",
    "rec_count_name = \"recipie_count.pth\"\n",
    "\n",
    "ing_acum_dist_name = \"ingredient_cumulative_distribution.pth\"\n",
    "rec_acum_dist_name = \"recipie_cumulative_distribution.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231142"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "    for data_len,row in enumerate(csv.reader(input_csvfile)):\n",
    "        pass\"\"\"\n",
    "data_len = 2231142\n",
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = ['\\x00']\n",
    "\n",
    "def fix_text(line, j):\n",
    "    return j.join([r for r in json.loads(line) if not all([r.__contains__(bad) for bad in bads])])\n",
    "if not os.path.exists(base_path + data_name):\n",
    "    with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        with open(base_path + data_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "            next(csv_reader, None)\n",
    "\n",
    "            # Copy selected columns from the input file to the output file\n",
    "            for row in (tqdm(csv_reader)):\n",
    "                row = [fix_text(row[2], \" * \"),fix_text(row[3], \" \")]\n",
    "                csv_writer.writerow(row)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(base_path + data_tokanized_name):\n",
    "    with open(base_path + data_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        with open(base_path + data_tokanized_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "        \n",
    "            for (ingrent_list, recipie) in tqdm(csv_reader, total=data_len):\n",
    "                ingrent_tokalized = [w.lower() for w in word_tokenize(ingrent_list)]\n",
    "                recipie_tokalized = [w.lower() for w in word_tokenize(recipie)]\n",
    "                csv_writer.writerow((ingrent_tokalized, recipie_tokalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(base_path + ing_count_name):\n",
    "    ingredients_count = Counter()\n",
    "    recipies_count = Counter()\n",
    "\n",
    "    with open(base_path + data_tokanized_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        csv_reader = csv.reader(input_csvfile)\n",
    "        for ingrent_tokalized, recipie_tokalized in tqdm(csv_reader, total=data_len):\n",
    "            for word in eval(ingrent_tokalized):\n",
    "                ingredients_count[word] += 1\n",
    "            for word in eval(recipie_tokalized):\n",
    "                recipies_count[word] += 1\n",
    "    torch.save(ingredients_count, base_path + ing_count_name)\n",
    "    torch.save(recipies_count, base_path + rec_count_name)\n",
    "ingredients_count = torch.load(base_path + ing_count_name)\n",
    "recipies_count =  torch.load(base_path + rec_count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 18681128), (',', 10728085), ('and', 9779961), ('the', 8766654), ('in', 5187219), ('to', 4966756), ('a', 4889946), ('with', 3344832), ('until', 2892925), ('add', 2730829), ('of', 2523757), ('minutes', 2393705), ('for', 2316425), ('or', 1644409), ('on', 1641420), ('into', 1608228), ('heat', 1585694), ('over', 1406082), (';', 1347366), ('mix', 1242485)]\n"
     ]
    }
   ],
   "source": [
    "print(recipies_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114312"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ingredients_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('*', 17274871),\n",
       " ('1', 7409472),\n",
       " (',', 4281177),\n",
       " ('2', 3349649),\n",
       " ('.', 3174623),\n",
       " ('1/2', 2863843),\n",
       " ('cup', 2473045),\n",
       " ('c.', 2417218),\n",
       " ('(', 2308674),\n",
       " (')', 2306500)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08287941257199803, 0.13047489946242902, 0.17386400437392316, 0.21275754087460752, 0.2357708012945515, 0.25780597067342664, 0.2795003700688374, 0.2943398225532924, 0.30717437540335835, 0.3192897842456176]\n",
      "[0.9999999600712166, 0.9999999645077481, 0.9999999689442796, 0.999999973380811, 0.9999999778173426, 0.9999999822538741, 0.9999999866904056, 0.999999991126937, 0.9999999955634685, 1.0]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(base_path + ing_acum_dist_name):\n",
    "    ing_acum_dist = []\n",
    "    suum = 0\n",
    "    s = sum(ingredients_count.values())\n",
    "    for w, c in ingredients_count.most_common():\n",
    "        suum += c\n",
    "        ing_acum_dist.append(suum / s)\n",
    "    rec_acum_dist = []\n",
    "    suum = 0\n",
    "    s = sum(recipies_count.values())\n",
    "    for w, c in recipies_count.most_common():\n",
    "        suum += c\n",
    "        rec_acum_dist.append(suum / s)\n",
    "    torch.save(ing_acum_dist, base_path + ing_acum_dist_name)\n",
    "    torch.save(rec_acum_dist, base_path + rec_acum_dist_name)\n",
    "ing_acum_dist = torch.load(base_path + ing_acum_dist_name)\n",
    "rec_acum_dist =  torch.load(base_path + rec_acum_dist_name)\n",
    "print(rec_acum_dist[:10])\n",
    "print(rec_acum_dist[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_larger_than(lst, x):\n",
    "    for i, item in enumerate(lst):\n",
    "        if item > x:\n",
    "            return i\n",
    "    return -1  # If no item is larger than x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114312 259014\n",
      "4767 8194\n",
      "183 166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'etc.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing_sum = sum(ingredients_count.values())\n",
    "rec_sum = sum(recipies_count.values())\n",
    "print(len(ingredients_count), len(recipies_count))\n",
    "ing_prob = 0.994\n",
    "rec_prob = 0.994\n",
    "ingredient_words = [\"<UKN>\"] + [t[0] for t in ingredients_count.most_common(find_index_larger_than(ing_acum_dist, ing_prob))]\n",
    "recipie_words = [\"<UKN>\"] + [t[0] for t in recipies_count.most_common(find_index_larger_than(rec_acum_dist, rec_prob))]\n",
    "print(len(ingredient_words), len(recipie_words))\n",
    "print(ingredients_count[ingredient_words[-1]], recipies_count[recipie_words[-1]])\n",
    "ingredient_words[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'small',\n",
       " 'jar',\n",
       " 'chipped',\n",
       " 'beef',\n",
       " ',',\n",
       " 'cut',\n",
       " 'up',\n",
       " '*',\n",
       " '4',\n",
       " 'boned',\n",
       " 'chicken',\n",
       " 'breasts',\n",
       " '*',\n",
       " '1',\n",
       " 'can',\n",
       " 'cream',\n",
       " 'of',\n",
       " 'mushroom',\n",
       " 'soup',\n",
       " '*',\n",
       " '1',\n",
       " 'carton',\n",
       " 'sour',\n",
       " 'cream']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(\"['1', 'small', 'jar', 'chipped', 'beef', ',', 'cut', 'up', '*', '4', 'boned', 'chicken', 'breasts', '*', '1', 'can', 'cream', 'of', 'mushroom', 'soup', '*', '1', 'carton', 'sour', 'cream']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 808905/2231142 [01:59<03:30, 6755.72it/s]"
     ]
    }
   ],
   "source": [
    "ing_word_to_id = dict(zip(ingredient_words, list(range(len(ingredient_words)))))\n",
    "rec_word_to_id = dict(zip(recipie_words, list(range(len(recipie_words)))))\n",
    "if not os.path.exists(base_path + prepeard_data_name):\n",
    "    with open(base_path + prepeard_data_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "        with open(base_path + data_tokanized_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            a = next(csv_reader)\n",
    "            csv_writer.writerow([\"X\",\"Y\"])\n",
    "\n",
    "            for ing, rec in tqdm(csv_reader, total=data_len):\n",
    "                ing_ids = [ing_word_to_id.get(i,0) for i in eval(ing)]\n",
    "                rec_ids = [rec_word_to_id.get(r,0) for r in eval(rec)]\n",
    "                csv_writer.writerow((ing_ids, rec_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(base_path + ingredient_vocab_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "    csv_writer = csv.writer(output_csvfile)\n",
    "\n",
    "    csv_writer.writerow([\"words\"])\n",
    "\n",
    "    # Copy selected columns from the input file to the output file\n",
    "    for row in ingredient_words:\n",
    "        csv_writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(base_path + recipie_vocab_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "    csv_writer = csv.writer(output_csvfile)\n",
    "\n",
    "    csv_writer.writerow([\"wors\"])\n",
    "\n",
    "    # Copy selected columns from the input file to the output file\n",
    "    for row in recipie_words:\n",
    "        csv_writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
