{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrmrk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveOnIterate:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        # This method should return the iterator object (in this case, self)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Check if there are more elements in the data\n",
    "        if self.data:\n",
    "            # Get and remove the next element from the data\n",
    "            result = self.data.pop(0)\n",
    "            return result\n",
    "        else:\n",
    "            # No more elements, raise StopIteration\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\\\\"\n",
    "original_cvs_name = r\"RecipeNLG_dataset.csv\"\n",
    "prepeard_data_name = \"prepeard_data.csv\"\n",
    "ingredient_vocab_name = \"ingredient_vocab.csv\"\n",
    "recipie_vocab_name = \"recipie_vocab.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229650it [00:04, 56312.21it/s]"
     ]
    }
   ],
   "source": [
    "bads = ['\\x00']\n",
    "data = []\n",
    "\n",
    "def fix_text(line, j):\n",
    "    return j.join([r for r in json.loads(line) if not all([r.__contains__(bad) for bad in bads])])\n",
    "\n",
    "with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        csv_reader = csv.reader(input_csvfile)\n",
    "        next(csv_reader, None)\n",
    "\n",
    "        # Copy selected columns from the input file to the output file\n",
    "        for i,row in enumerate(tqdm(csv_reader)):\n",
    "            data.append([fix_text(row[2], \" * \"),fix_text(row[3], \" \")])\n",
    "            if i == 10000:\n",
    "                pass\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'c.', 'firmly', 'packed', 'brown', 'sugar', '*', '1/2', 'c.', 'evaporated', 'milk', '*', '1/2', 'tsp', '.', 'vanilla', '*', '1/2', 'c.', 'broken', 'nuts', '(', 'pecans', ')', '*', '2', 'Tbsp', '.', 'butter', 'or', 'margarine', '*', '3', '1/2', 'c.', 'bite', 'size', 'shredded', 'rice', 'biscuits']\n",
      "['In', 'a', 'heavy', '2-quart', 'saucepan', ',', 'mix', 'brown', 'sugar', ',', 'nuts', ',', 'evaporated', 'milk', 'and', 'butter', 'or', 'margarine', '.', 'Stir', 'over', 'medium', 'heat', 'until', 'mixture', 'bubbles', 'all', 'over', 'top', '.', 'Boil', 'and', 'stir', '5', 'minutes', 'more', '.', 'Take', 'off', 'heat', '.', 'Stir', 'in', 'vanilla', 'and', 'cereal', ';', 'mix', 'well', '.', 'Using', '2', 'teaspoons', ',', 'drop', 'and', 'shape', 'into', '30', 'clusters', 'on', 'wax', 'paper', '.', 'Let', 'stand', 'until', 'firm', ',', 'about', '30', 'minutes', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(data[0][0]))\n",
    "print(word_tokenize(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 885836/2231142 [12:02<18:17, 1225.53it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ingrent_tokalized:\n\u001b[0;32m      7\u001b[0m     ingredients_count[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m recipie_tokalized \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipie\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m recipie_tokalized:\n\u001b[0;32m     10\u001b[0m     recipies_count[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\mrmrk\\OneDrive\\Documents\\GitHub\\recipe_generation\\.venv\\lib\\site-packages\\nltk\\tokenize\\destructive.py:178\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    175\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[1;32m--> 178\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m    181\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:326\u001b[0m, in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[43m_compile_repl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m template[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ingredients_count = Counter()\n",
    "recipies_count = Counter()\n",
    "data_tokanized = []\n",
    "for (ingrent_list, recipie) in tqdm(RemoveOnIterate(data)):\n",
    "    ingrent_tokalized = [w.lower() for w in word_tokenize(ingrent_list)]\n",
    "    for word in ingrent_tokalized:\n",
    "        ingredients_count[word] += 1\n",
    "    recipie_tokalized = [w.lower() for w in word_tokenize(recipie)]\n",
    "    for word in recipie_tokalized:\n",
    "        recipies_count[word] += 1\n",
    "    data_tokanized.append((ingrent_tokalized, recipie_tokalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recipies_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_tokanized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_sum = sum(ingredients_count.values())\n",
    "rec_sum = sum(recipies_count.values())\n",
    "print(ing_sum, rec_sum)\n",
    "ing_prob = 0.01/100\n",
    "rec_prob = 0.01/100\n",
    "ingredient_words = [\"<UKN>\"] + [w for w, c in ingredients_count.most_common() if c/ing_sum >= ing_prob]\n",
    "recipie_words = [\"<UKN>\"] + [w for w, c in recipies_count.most_common() if  c/rec_sum >= rec_prob]\n",
    "print(len(ingredient_words), ingredient_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(vocab, word):\n",
    "    try:\n",
    "        return vocab.index(word)\n",
    "    except:\n",
    "        return 0\n",
    "data_ids = []\n",
    "for ing, rec in tqdm(RemoveOnIterate(data_tokanized)):\n",
    "    ing_ids = [get_id(ingredient_words, i) for i in ing]\n",
    "    rec_ids = [get_id(recipie_words, r) for r in rec]\n",
    "    data_ids.append((ing_ids, rec_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(base_path + prepeard_data_name):\n",
    "    with open(base_path + prepeard_data_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "        csv_writer = csv.writer(output_csvfile)\n",
    "\n",
    "        csv_writer.writerow([\"X\",\"Y\"])\n",
    "\n",
    "        # Copy selected columns from the input file to the output file\n",
    "        for row in data_ids:\n",
    "            csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(base_path + ingredient_vocab_name):\n",
    "    with open(base_path + ingredient_vocab_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "        csv_writer = csv.writer(output_csvfile)\n",
    "\n",
    "        csv_writer.writerow([\"words\"])\n",
    "\n",
    "        # Copy selected columns from the input file to the output file\n",
    "        for row in ingredient_words:\n",
    "            csv_writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(base_path + recipie_vocab_name):\n",
    "    with open(base_path + recipie_vocab_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "        csv_writer = csv.writer(output_csvfile)\n",
    "\n",
    "        csv_writer.writerow([\"wors\"])\n",
    "\n",
    "        # Copy selected columns from the input file to the output file\n",
    "        for row in recipie_words:\n",
    "            csv_writer.writerow([row])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
