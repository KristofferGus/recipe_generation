{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveOnIterate:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.original_len = len(data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # This method should return the iterator object (in this case, self)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Check if there are more elements in the data\n",
    "        if self.data:\n",
    "            # Get and remove the next element from the data\n",
    "            result = self.data.pop(0)\n",
    "            return result\n",
    "        else:\n",
    "            # No more elements, raise StopIteration\n",
    "            raise StopIteration\n",
    "    def __len__(self):\n",
    "        return self.original_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\KG\\Documents\\GitHub\\recipe_generation\\\\\"\n",
    "original_cvs_name = r\"RecipeNLG_dataset.csv\"\n",
    "prepeard_data_name = \"prepeard_data2.csv\"\n",
    "vocab_name = \"vocab.csv\"\n",
    "\n",
    "\n",
    "data_name = \"data.csv\"\n",
    "data_tokanized_name = \"data_tokanized.csv\"\n",
    "\n",
    "count_name = \"count.pth\"\n",
    "\n",
    "\n",
    "acum_dist_name = \"cumulative_distribution.pth\"\n",
    "\n",
    "\n",
    "data_tensor_name = \"data_tensor.pth\"\n",
    "data_tensor_len_name = \"data_len_tensor.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231142"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "    for data_len,row in enumerate(csv.reader(input_csvfile)):\n",
    "        pass\"\"\"\n",
    "data_len = 2231142\n",
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = ['\\x00']\n",
    "\n",
    "def fix_text(line, j):\n",
    "    return j.join([r for r in json.loads(line) if not all([r.__contains__(bad) for bad in bads])])\n",
    "if not os.path.exists(base_path + data_name):\n",
    "    with open(base_path + original_cvs_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        with open(base_path + data_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "            next(csv_reader, None)\n",
    "\n",
    "            # Copy selected columns from the input file to the output file\n",
    "            for row in (tqdm(csv_reader)):\n",
    "                row = [fix_text(row[2], \" * \"),fix_text(row[3], \" \")]\n",
    "                csv_writer.writerow(row)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(base_path + data_tokanized_name):\n",
    "    with open(base_path + data_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        with open(base_path + data_tokanized_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "        \n",
    "            for (ingrent_list, recipie) in tqdm(csv_reader, total=data_len):\n",
    "                ingrent_tokalized = [w.lower() for w in word_tokenize(ingrent_list)]\n",
    "                recipie_tokalized = [w.lower() for w in word_tokenize(recipie)]\n",
    "                csv_writer.writerow((ingrent_tokalized, recipie_tokalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(base_path + count_name):\n",
    "    count = Counter()\n",
    "\n",
    "\n",
    "    with open(base_path + data_tokanized_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "        csv_reader = csv.reader(input_csvfile)\n",
    "        for ingrent_tokalized, recipie_tokalized in tqdm(csv_reader, total=data_len):\n",
    "            for word in eval(ingrent_tokalized):\n",
    "                count[word] += 1\n",
    "            for word in eval(recipie_tokalized):\n",
    "                count[word] += 1\n",
    "    torch.save(count, base_path + count_name)\n",
    "    \n",
    "count = torch.load(base_path + count_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 21855751), ('*', 17348785), (',', 15009262), ('and', 10618867), ('the', 8904360), ('1', 8468765), ('to', 5503526), ('in', 5352003), ('a', 4986816), ('2', 4161585), ('with', 3440550), ('1/2', 3306458), (')', 3206781), ('(', 3192694), ('of', 2962607), ('cup', 2943471), ('until', 2900352), ('add', 2742097), ('for', 2595950), ('or', 2576990)]\n"
     ]
    }
   ],
   "source": [
    "print(count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06290745189193798, 0.11284249451623743, 0.15604368340233396, 0.18660798956364535, 0.21223742679179172, 0.2366130900587107, 0.25245389997098644, 0.2678585809287814, 0.2822121434128135, 0.29419044185617105]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(base_path + acum_dist_name):\n",
    "    acum_dist = []\n",
    "    suum = 0\n",
    "    s = sum(count.values())\n",
    "    for w, c in count.most_common():\n",
    "        suum += c\n",
    "        acum_dist.append(suum / s)\n",
    "    \n",
    "  \n",
    "    torch.save(acum_dist, base_path + acum_dist_name)\n",
    "\n",
    "acum_dist = torch.load(base_path + acum_dist_name)\n",
    "\n",
    "print(acum_dist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_larger_than(lst, x):\n",
    "    for i, item in enumerate(lst):\n",
    "        if item > x:\n",
    "            return i\n",
    "    return -1  # If no item is larger than x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314174\n",
      "1500\n",
      "12729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'crusty'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing_sum = sum(count.values())\n",
    "print(len(count))\n",
    "prob = 0.95318\n",
    "words = [\"<UKN>\", \"<PAD>\"] + [t[0] for t in count.most_common(find_index_larger_than(acum_dist, prob))]\n",
    "print(len(words))\n",
    "print(count[words[-1]])\n",
    "words[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2231141/2231142 [09:21<00:00, 3976.76it/s]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = dict(zip(words, list(range(len(words)))))\n",
    "\n",
    "if not os.path.exists(base_path + prepeard_data_name):\n",
    "    with open(base_path + prepeard_data_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "        with open(base_path + data_tokanized_name, 'r', newline='', encoding='utf-8') as input_csvfile:\n",
    "            csv_reader = csv.reader(input_csvfile)\n",
    "            csv_writer = csv.writer(output_csvfile)\n",
    "            a = next(csv_reader)\n",
    "            csv_writer.writerow([\"X\",\"Y\"])\n",
    "\n",
    "            for ing, rec in tqdm(csv_reader, total=data_len):\n",
    "                ing_ids = [word_to_id.get(i,0) for i in eval(ing)]\n",
    "                rec_ids = [word_to_id.get(r,0) for r in eval(rec)]\n",
    "                csv_writer.writerow((ing_ids, rec_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open(base_path + prepeard_data_name, 'r') as file:\n",
    "        next(csv.reader(file))\n",
    "        maax_len = 0\n",
    "        for i, (ing, rec) in enumerate(tqdm(csv.reader(file), total=data_len)):\n",
    "            l = max(len(eval(ing)), len(eval(rec)))\n",
    "            if l > maax_len:\n",
    "                maax_len = l\"\"\"\n",
    "maax_len = 3016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(base_path + vocab_name, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "    csv_writer = csv.writer(output_csvfile)\n",
    "\n",
    "    csv_writer.writerow([\"words\"])\n",
    "\n",
    "    # Copy selected columns from the input file to the output file\n",
    "    for row in words:\n",
    "        csv_writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
